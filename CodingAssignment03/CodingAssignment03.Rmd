---
title: "Coding Assignment 3"
author: "Team 9"
date: "Due: 2024-12-07 23:59"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
#Put any packages you need here
knitr::opts_chunk$set(echo = TRUE)
library(dplyr) # for grammar of data manipulation
library(jtools) # for analysis and presentation of data
library(car) # for vif function
library(plotly) # for interactive visualizations
library(gt) # for better looking tables
library(gtsummary) # for better summary statistics
```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 100 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan. 

The data on the 100 customers in the sample is as follows:

-	Charges: Total medical expenses for a particular insurance plan (in dollars)
-	Age: Age of the primary beneficiary
-	BMI: Primary beneficiary’s body mass index (kg/m2)
-	Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-	Children: Number of children covered by health insurance plan (includes other dependents as well)
-	Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-	Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.


```{r dataset}
insurance <- read.csv("../CodingAssignment03/Insurance_Data_Group9.csv")
gt(head(insurance)) # the gt function only makes it look nicer
```



## Question 1

Randomly select 30 observations from the sample and exclude from all modeling. Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 70 observations.

```{r q1}

set.seed(123457)
exclude <- sample(nrow(insurance), 30)
train <- insurance[-exclude,] # data for 30 excluded observations
test <- insurance[exclude,] # data for 70 included observations

train %>%
  select(Charges, Age, BMI, Children) %>%
  tbl_summary(statistic = list(all_continuous() ~ c("{mean}",       # Mean
                                                    "{sd}",         # Standard Deviation
                                                    "{median}",     # Median
                                                    "{min}",        # Minimum
                                                    "{max}"         # Maximum
                                                    )
                               ),
    type = all_continuous() ~ "continuous2" # Enhanced formatting for continuous variables
  )

```


## Question 2

Provide the correlation between all quantitative variables

```{r}
cor(train[, c("Charges", "Age", "BMI", "Children")])
```


## Question 3

Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?

```{r q3}
allvar <- lm(Charges ~ Age + BMI + Female + Children + Smoker + WinterPark + WinterSprings + Oviedo, data = train)
summary(allvar)
plot(allvar)
```

Yes, the plots above show some of the Gauss-Markov assumptions have been violated.

The ‘Residuals versus Fitted’ graph shows that there is a non-linear relationship between the variables. This violates assumption 3 of the Gauss-Markov theorem, which states that independent variables are not correlated with error. This nonlinear relationship also show that there is specification bias.

The second plot titled “Normal Q-Q” shows the assumption of a normally distributed dependent variable for a fixed set of predictors. The line shown is fairly a 45-degree line upwards, although there are some observations which fall above and below the line. This makes it non-linear, which violates 6th assumption of the Gauss-Markov assumptions.

Finally, the third plot titled “Scale-Location” checks for homoskedasticity, which is shown on this graph and violates assumption 4 of the Gauss-Markov theorem because the data spreads apart as time increases.If this assumption were not violated, you’d see random points around a horizontal line. In this case, the line is not horizontal and the results tend to trend downward sloping, with a slight “fanning out” effect.

The last plot “Residuals vs. Leverage” keeps an eye out for regression outliers, influential observations, and high leverage points. There currently aren’t any outliers which fall outside Cook’s distance.

These should be solutions for the Gauss Markov violations:

For non-linearity, you can transform certain independent or dependent variables by using log transformations, or square root transformations. Due to the spread of data for insurance charges, taking the logarithmic function of Charges will allow us to evaluate the data by making it linear as well as stabilize variance for Heteroscedasticity.

You also could modify the model to include dummy variables that allow for different intercepts for each group, it could offer a more accurate representation of the data and alleviate the issues caused by heteroscedasticity.Most insurance do not count the number of children in a family, there are typically two types of insurance options, ‘single’ and ‘family’. So transforming the variable of Children from a quantitative to a dummy variable helps to better categorize the variable and evaluate the data.

## Question 4

Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data and run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?

```{r q4}
hist(train$Charges) #before
train$lnCharges <- log(train$Charges) 
hist(train$lnCharges) #after for log Charges

hist(train$Charges) #before
train$ChargesSquared <- train$Charges^2 # charges squared
hist(train$ChargesSquared) #after

scatterplotMatrix(train[c(10,2:3,5)]) # lnCharges
par(mfrow=c(1,2)) # Charges side by side
```

As seen above, taking logs will bring outliers closer to other charges. After transforming charges into the log function, the histograms above show that the log version brings charges closer to a normal distribution.

Below, we will take both the log and quadratic forms of multiple variables to determine better fits relative to the linear form, which was found to be problematic.


```{r}
train$ChildrenSquared <- train$Children^2 # Children squared
hist(train$ChildrenSquared) #after
summary(train$ChildrenSquared)

train$lnChildren <- log(train$Children) #log of children
hist(train$lnChildren) #after
summary(train$lnChildren)

scatterplotMatrix(train[c(10,2:3,12)]) # lnCharges with lnChildren
par(mfrow=c(1,2)) # Charts side by side

```

```{r}
hist(train$Age) #before
train$lnAge <- log(train$Age) # log of age
hist(train$lnAge) #after
summary(train$lnAge)

hist(train$Age) #before
train$AgeSquared <- train$Age^2 # age squared
hist(train$AgeSquared) #after
summary(train$lnAge)
```


```{r}
hist(train$BMI) #before
train$lnBMI <- log(train$BMI) #log of BMI
hist(train$lnBMI) #after
summary(train$lnBMI)

train$BMISquared <- train$BMI^2 # quadratic of BMI
hist(train$BMISquared)  #after
summary(train$BMISquared)
```



```{r}
#Model 1
model_1 <- lm(lnCharges ~., data = train[,c(10,2:9)] ) #pulling only columns I want
summary(model_1)

par(mfrow=c(2,2))
plot(model_1)

#ln charges
```



```{r}
#Model 2
model_2 <- lm(lnCharges ~., data = train[,c(10,3:9,15)] ) #pulling only columns I want

summary(model_2)

par(mfrow=c(2,2))
plot(model_2)

#lnCharges and AgeSquared
```

```{r}
#Model 3
model_3 <- lm(lnCharges ~., data = train[,c(10,14,3:9)] ) #pulling only columns I want

summary(model_3)

par(mfrow=c(2,2))
plot(model_3)

#lnCharges and lnAge
```

```{r}
#Model 4
model_4 <- lm(lnCharges ~., data = train[,c(10,2,4:9,17)] ) #pulling only columns I want

summary(model_4)

par(mfrow=c(2,2))
plot(model_4)

#lnCharges and BMISquared
```

```{r}
#Model 5
model_5 <- lm(lnCharges ~., data = train[,c(10,2,4:9,16)] ) #pulling only columns I want
summary(model_5)

par(mfrow=c(2,2))
plot(model_5)

#lnCharges and lnBMI

```

```{r}
#Model 6
model_6 <- lm(lnCharges ~., data = train[,c(10,2:4,6:9,12,5)] ) #pulling only columns I want

summary(model_6)

par(mfrow=c(2,2))
plot(model_6)

#lnCharges and ChildrenSquared
```

```{r}
#make children a dummy variable
train$childdummy <- 0 
train$childdummy[train$Children > 0] <- 1
```

```{r}
#Model 7
model_7 <- lm(lnCharges ~., data = train[,c(10,2:4,6:9,18)] ) #pulling only columns I want
summary(model_7)

#dummy variable children

```

```{r,warning=FALSE}
scatterplotMatrix(train[c(10,2:3,18)]) # pulling lnCharges
par(mfrow=c(1,2))
```

```{r}
par(mfrow=c(2,2))
plot(model_7)
```


## Question 5

Use the 30 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

```{r q5}

bad_model <- lm(Charges ~., data = insurance)
summary(bad_model)

test$lnCharges <- log(test$Charges)
test$lnAge <- log(test$Age)
test$lnBMI <- log(test$BMI)
```

```{r}
test$bad_model_pred <- predict(bad_model, newdata = test)

test$lnCharges <- log(test$Charges)
test$lnage <- log(test$Age)
test$lnBMI <- log(test$BMI)

test$model_3_pred <- predict(model_3, newdata = test) %>% exp()
#lnAge

test$model_5_pred <- predict(model_5, newdata = test) %>% exp()
#lnBMI

# Finding the error

test$error_bm <- test$bad_model_pred - test$Charges

test$error_3 <- test$model_3_pred - test$Charges

test$error_5 <- test$model_5_pred - test$Charges
```

### Bias

```{r}
# Bad Model
mean(test$error_bm)

# Model 1
mean(test$error_3)

# Model 2
mean(test$error_5)
```

### MAE


```{r}
# Function to calculate MAE

mae <- function(error_vector){
  error_vector %>% 
  abs() %>% 
  mean()
}

# Bad Model
mae(test$error_bm)

# Model 1
mae(test$error_3)

# Model 2
mae(test$error_5)

```

### RMSE

```{r}
rmse <- function(error_vector){
   error_vector^2 %>% 
  mean() %>% 
  sqrt()

}

# Bad Model
rmse(test$error_bm)
# Model 1
rmse(test$error_3)

# Model 2
rmse(test$error_5)
```

### MAPE

```{r}
mape <- function(error_vector, actual_vector){
  (error_vector/actual_vector) %>% 
    abs() %>% 
    mean()
}

# Bad Model
mape(test$error_bm, test$saleprice)

# Model 1
mape(test$error_3, test$saleprice)

# Model 2
mape(test$error_5, test$saleprice)
```

All things considered, though Model 2 has the lowest MAE, Model 1 is the least biased and has the lowest RMSE. Therefore, it would be the best model to use long-term.


## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.

```{r q6}
#
```

## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
| -------- | --- | --- | ------ | -------- | ------ | -------------- | 
| 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
| 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
| 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
| 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
| 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |


```{r q7}
#
```

## Question 8

The owner notices that some of the predictions are wider than others, explain why.

The prediction and confidence outputs reveal that the prediction interval has wider boundaries than the confidence interval, which is expected. This difference arises because the prediction interval accounts for both the uncertainty in estimating the population mean and the random variation of individual values.

In this case, some prediction intervals are wider than others due to the use of independent variable values that are significantly larger or smaller than average, placing them closer to the bounds of the range. For instance, comparing the prediction intervals for row 1 (which uses the largest age and lowest BMI) and row 3 (which uses the same values for age and BMI) highlights this variation. Row 1 has a wider prediction interval of 10.01 - 8.11 = 1.90, while row 3 has a slightly narrower prediction interval of 10.44 - 8.62 = 1.82. This difference illustrates how the input data can influence the width of the prediction intervals.


## Question 9 

Are there any prediction problems that occur with the five potential clients? If so, explain.

Prediction problems often involve several types of errors, including extrapolation, model bias, high variance, and the effects of non-linearity and interactions. In this context, extrapolation error is unlikely to be a concern since the range of values for each independent variable in Model 7’s training data set adequately covers the range needed for the five clients in terms of age, BMI, number of children, and other factors. However, the model exhibits a negative bias, suggesting that predictions are likely to underestimate actual values.

Another critical consideration is the sample size. Model 7’s training dataset includes only 30 observations, which is relatively small. A larger sample size, such as 100 or more, would provide a more accurate representation and improve the model’s reliability. Additionally, when using a model with a log-transformed dependent variable like lncharges, it is crucial to back-transform the results to reflect the true dollar amounts for accurate interpretation.
